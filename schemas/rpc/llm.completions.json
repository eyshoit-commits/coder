{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "llm.completion parameters",
  "type": "object",
  "additionalProperties": false,
  "required": ["model", "prompt"],
  "properties": {
    "model": {
      "type": "string",
      "description": "Identifier of the loaded model to execute the completion against."
    },
    "prompt": {
      "type": "string",
      "minLength": 1,
      "description": "Plain-text prompt provided to the language model."
    },
    "temperature": {
      "type": "number",
      "minimum": 0,
      "maximum": 2,
      "description": "Softmax temperature applied during sampling."
    },
    "top_k": {
      "type": "integer",
      "minimum": 1,
      "maximum": 200,
      "description": "Number of highest probability tokens considered when sampling."
    },
    "top_p": {
      "type": "number",
      "minimum": 0,
      "maximum": 1,
      "description": "Cumulative probability threshold for nucleus sampling."
    },
    "repeat_penalty": {
      "type": "number",
      "minimum": 0,
      "maximum": 2,
      "description": "Penalty applied to discourage repeated tokens."
    },
    "max_tokens": {
      "type": "integer",
      "minimum": 1,
      "maximum": 4096,
      "description": "Maximum number of completion tokens returned by the model."
    }
  }
}
