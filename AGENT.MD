### ğŸ§  Codex-Systemprompt: Entwickle das CyberDevStudio

Du bist Codex, ein autonomer Entwicklungsagent. Du baust ein vollstÃ¤ndiges, modulares, agentengesteuertes Dev-Studio mit eingebautem LLM-Server (basierend auf `node-llama-cpp`, max. 3B GGUF-Modelle). Die Plattform ermÃ¶glicht Nutzern Ã¼ber ein Web-Frontend Code zu schreiben, LLM-Modelle zu starten, Telemetrie zu Ã¼berwachen, Projekte zu organisieren und Token-gesteuert inferenzielle Dienste zu nutzen.

Das Projekt basiert auf Rust (Backend + Execution), TypeScript (Frontend), PostgreSQL + PostgresML (Nutzer- & Tokenverwaltung), node-llama-cpp (LLM-Inferenz) und besteht aus mehreren Modulen: Agenten, UI, Sandbox, Modelhosting, Admin-Dashboard, User-System, Tracing, Metrics, CI/CD, Auth. Es nutzt moderne Technologien wie WebSockets, OpenTelemetry, Prometheus, Docker Compose, JSON-RPC.

---

## ğŸ§© Verzeichnisstruktur

```
CyberDevStudio/
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ studio-ui/            # Monaco IDE, AgentChat, AdminPanel
â”‚   â”œâ”€â”€ api/                  # JSON-RPC Gateway, Auth, ProjectStore
â”‚   â”œâ”€â”€ llmserver/            # node-llama-cpp Wrapper mit Tokenkontrolle
â”‚   â””â”€â”€ auth/                 # Login, API-Key, Tokens, UserRoles
â”œâ”€â”€ schemas/rpc/              # JSON-RPC Call Schemas
â”œâ”€â”€ database/
â”‚   â””â”€â”€ migrations/           # PostgresML + Token Tables
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.api
â”‚   â”œâ”€â”€ Dockerfile.llm
â”‚   â”œâ”€â”€ Dockerfile.ui
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ sandbox/
â”‚   â”œâ”€â”€ fs.rs
â”‚   â”œâ”€â”€ run.rs
â”‚   â”œâ”€â”€ wasm.rs
â”‚   â””â”€â”€ micro.rs
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ fs_write.rs
â”‚   â”œâ”€â”€ run_exec.rs
â”‚   â””â”€â”€ e2e.rs
â”œâ”€â”€ examples/rpc/
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ otel-config.yaml
â”‚   â””â”€â”€ prometheus.yml
â”œâ”€â”€ themes/
â”‚   â”œâ”€â”€ NeonCyberNight.css
â”‚   â””â”€â”€ SerialSteel.css
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ acceptance.md
â”‚   â”œâ”€â”€ Projektplan.md
â”‚   â””â”€â”€ API.md
â””â”€â”€ README.md
```

---

## ğŸ” Benutzer & Rollen

* PostgreSQL mit [PostgresML](https://github.com/postgresml/postgresml)
* Tabellen:

  * `users` (id, username, role, api_key_hash, balance)
  * `tokens_used` (user_id, timestamp, model_id, tokens)
  * `models` (id, name, context_size, cost_per_token)
* Rollen: `admin`, `developer`, `viewer`
* Token-Abrechnung beim LLM-Zugriff via Middleware

---

## ğŸ“Š Admin-Panel (UI + API)

VerfÃ¼gbar unter `/admin`, nur fÃ¼r `admin`-User via JWT:

* ModellÃ¼bersicht (VerfÃ¼gbare, Geladene, RAM-Verbrauch)
* Modellaktionen:

  * **Download** von HugginFace (max. 3B)
  * **Start**, **Stop**, **Unload**
  * TokenLimit, ContextSize, Threads, Temp, TopK
* User-Verwaltung: User anlegen, Tokens setzen
* Model-Zugriff einschrÃ¤nken per API-Key
* Logs: Request-Log, Errors, Token-History
* Systemstatus: CPU, RAM, Last Load, Active Sessions
* `/metrics`: OpenTelemetry & Prometheus Export

---

## ğŸ”Œ node-llama-cpp API (eingebaut)

| Route                       | Beschreibung                      |
| --------------------------- | --------------------------------- |
| `POST /v1/chat/completions` | OpenAI-kompatible Chat-API        |
| `POST /v1/completions`      | Klassische Prompt Completion      |
| `POST /v1/embeddings`       | Embedding Generierung             |
| `POST /admin/load`          | LÃ¤dt Modell aus `/models`         |
| `POST /admin/unload`        | Entfernt aktives Modell           |
| `GET  /admin/status`        | Infos Ã¼ber RAM, Tokens, Threads   |
| `GET  /admin/models`        | Listet verfÃ¼gbare GGUF-Modelle    |
| `GET  /metrics`             | OTEL-kompatible Prometheus Metrik |

#### Modellquellen (nur â‰¤ 3B)

| Modelltyp | Name                  | Huggingface URL                                                                                                                                                  |
| --------- | --------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Coding    | `deepseek-coder-1.3b` | [https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-instruct-GGUF](https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-instruct-GGUF)                     |
| Chat      | `nous-hermes-2-3b.Q4` | [https://huggingface.co/TheBloke/Nous-Hermes-2-3B-GGUF](https://huggingface.co/TheBloke/Nous-Hermes-2-3B-GGUF)                                                   |
| Embedding | `bge-small-en-v1.5`   | [https://huggingface.co/BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)                                                                   |
| Function  | `tinyllama-1.1b-func` | [https://huggingface.co/cognitivecomputations/TinyLlama-1.1B-Function-Call-GGUF](https://huggingface.co/cognitivecomputations/TinyLlama-1.1B-Function-Call-GGUF) |

---

## ğŸ§ª Testing

* Unit: `sandbox/*`, `auth/*`, `rpc::*`, `llmserver::*`
* E2E: Start Server â†’ RPC `fs.write`, `run.exec`, `llm.chat`
* Fehlerpfade: 401 Auth, 403 Policy, 429 Rate-Limit, 500 ModelCrash
* Tokenlimits testbar via Admin-Simulation

---

## ğŸ§  Inspirationsquellen (Analyse & Integration)

| Quelle                       | Feature                   | Status                       |
| ---------------------------- | ------------------------- | ---------------------------- |
| `Decentralised-AI/bolt.diy`  | Editor, FileLock, Diffing | ğŸŸ¢ UI-Komponenten integriert |
| `we0-dev/we0`                | Terminal via WebContainer | ğŸŸ¢ Ã¼bernommen                |
| `blissito/replit_clone`      | IDE Panels                | ğŸŸ¢ Editorbasis               |
| `AI-DevEnv-AutoConfigurator` | DevEnv + LLM Setup        | âœ… Konfiguration 1:1          |
| `microsandbox/microsandbox`  | Python/Node VMs           | âœ… Engine Ã¼bernommen          |
| `Visual-Prompt-Craft`        | Prompt BlÃ¶cke + UX        | ğŸ”„ UI-Flow Inspiration       |
| `litechain`, `ChainForge`    | Prompt Tools              | ğŸ”„ Prompt Logging Logik      |

---

## ğŸš€ Deployment: Docker Compose (keine Standardports)

```yaml
version: "3.9"
services:
  api:
    build: ./docker/Dockerfile.api
    ports: ["6813:6813"]
    environment:
      - RPC_PORT=6813
    depends_on: [db, llmserver]

  llmserver:
    build: ./docker/Dockerfile.llm
    ports: ["6988:6988"]
    environment:
      - LLM_PORT=6988
    volumes:
      - ./models:/models
      - ./logs:/logs

  studio-ui:
    build: ./docker/Dockerfile.ui
    ports: ["6711:6711"]

  auth:
    build: ./docker/Dockerfile.auth
    ports: ["6971:6971"]

  db:
    image: postgresml/postgresml
    restart: always
    environment:
      - POSTGRES_USER=admin
      - POSTGRES_PASSWORD=supersecure
      - POSTGRES_DB=cyberstudio
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports: ["6472:5432"]

volumes:
  pgdata:
```

---

## ğŸ§  Aufgabenpipeline fÃ¼r Codex

1. `Projektplan.md` anlegen (Engines, Ports, Tokensystem, Adminlogik)
2. `schemas/rpc/*.json` fÃ¼r alle RPC-Aktionen
3. `fs.rs`, `wasm.rs`, `micro.rs`, `run.rs` implementieren
4. Authsystem: API-Key Middleware, JWT Auth, PostgreSQL Tabellen
5. LLMServer Wrapper bauen (Rust â†’ `node-llama-cpp`)
6. Admin-UI bauen: Settings, Modelstatus, User-Management
7. UI-Modul: Editor, Chat, Terminal, Metrics im CyberNightDesign
8. Tokenverfolgung in PostgreSQL loggen
9. Prometheus & OTEL aktivieren
10. CI: fmt â†’ clippy â†’ test â†’ e2e â†’ artifact
11. Akzeptanztests schreiben, Payloads erzeugen

---
